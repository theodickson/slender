\chapter{Experimental results} \label{results}

In this section we discuss the experiments we performed to ascertain the performance benefits of shredding. First we discuss the testing setup and introduce the dataset used. Then we describe the experiments done and discuss their results. These results show that queries can be executed up to 3.7 times faster using shredding, but that not all queries are sped up by shredding.

\section{Setup}

Unfortunately, access to a distributed computing cluster was not secured, which made testing of the framework on static queries difficult, as without significant parallelisation, it would have been hard to test the load-balancing benefits of shredding. However, even without parallelisation, the incrementalisation benefits would be achievable, and so it was decided to test on streamed queries using DStreams.

The machine we use has 8GB 1600Mhz DDR3 Ram, a 2.6GHZ Intel Core i5 processor and runs Mac OS X 10.11.6. The queries are compiled using Scala 2.11.12 and are run on a local instance of Spark 2.3.0. 

The dataset used is that supplied with the TPC-H \cite{tpch} database benchmark tool. While the queries are not related to those in the benchmark, the dataset has a standard relational schema modelling a business?s products, orders, customers and suppliers. In order to execute queries successfully on our restricted setup, we sample this dataset by first taking a random sample of the customers, and then sampling the other tables based on this sample of customers. I.e. we take all the orders from our sample customers, and all the suppliers required to fulfill those orders. This is to retain the original distributions of the data.

\section{Isolated testing of group operator}

\begin{table}[]
\begin{tabular}{|l|l|l|l|l|}
\hline
               &             & \multicolumn{3}{l|}{Processing time (seconds) \textit{(speedup)}} \\ \hline
Query          & nested-size & reg          & shred-inc           & shred-acc           \\ \hline
group-lineitem & 3.93        & 18.3         & 4.38 \textit{(4.18\texttimes)}        & 6.74 \textit{(2.72\texttimes)}        \\ \hline
group-customer & 400         & 9.20         & 6.92 \textit{(1.33\texttimes)}        & 8.72 \textit{(1.06\texttimes)}        \\ \hline
group-orders   & 14.8        & 15.1         & 9.05 \textit{(1.67\texttimes)}        & 11.1 \textit{(1.36\texttimes}        \\ \hline
\end{tabular}
\caption{Test results of isolated group queries}
\label{groupresults}
\end{table}

First we test the performance of shredding the group operator in isolation. This is so we can directly assess the benefits of being able to avoid recomputation of the construction of nested collections without the additional overhead of joins which other operators will introduce into the query. We will test this operator across a variety of tables with different distributions of keys, in order to see how performance relates to the size of nested collections. For each query, we will compare three alternative processing methods:

\begin{itemize}
\item{Regular key-nested evaluation. This will involve recomputation with each batch in the DStream, and therefore will have an accumulated output.}
\item{Shredded evaluation. As this does not involve recomputation, the output will be incremental.}
\item{Shredded evaluation with accumulation. This will be shredded evaluation, with the output explicitly accumulated after application of the group operator.}
\end{itemize}

The idea here is that although we expect shredding without accumulation to be the quickest processing method, it is in some sense not comparable to the regular key-nested evaluation, since in order to compute the full result the RDDs in the DStream need to be aggregated. However, we note that the penalty of this aggregation is dependent on how the output is to be used. If it is being saved in permanent storage, i.e. a database on-disk, whether this aggregation actually incurs a significant cost depends on the update primitives available.

We use a sample based on 10000 randomly chosen customers.

Figure \ref{groupresults} shows the results of this isolated testing of group. The queries are described fully in appendix \ref{queries} but we illustrate the \textit{group-lineitem} query here:
\vs\begin{lstlisting}
Group(
  For ((l_orderkey,l_partkey,l_suppkey) <-- lineitem)
    Yield (l_orderkey,l_partkey)
)
\end{lstlisting}\vs

This query projects lineitem to just the order key and part key using a for-comprehension, to create a $\Bag{\Int \times \Int}$. It then then applies the group operator to this, to produce a collection of type $\coll{(\Int \times [\coll{\Int}{\Int}])}{\prim{Bool}}$. This represents order keys paired with an inner bag of the part keys from that order.

Firstly, it is notable that indeed shredding is quicker in every case, and further that the accumulated method while slower than incremental shredding is faster than regular evaluation. This is at least evidence that avoiding the recomputation of the inner collections in the shredded versions is beneficial.

However, it offers significant benefit in the query \textit{group-lineitem}. The others show less than a 2x gain in speed. This seems to be related to the size of the inner collections created. As the mean size of inner collection increases, the speed decreases relative to standard evaluation. It is hard to say exactly why this is happening but may be to do with overheads involved in computing the dictionary. Especially in our low-parallelism environment, any such overheads would not be offset by the load-balancing benefit we'd otherwise expect from such large inner collections.

\section{Complete queries}

\begin{table}[]
\begin{tabular}{|l|l|l|l|l|}
\hline
Query & \# batches & \multicolumn{3}{l|}{Time (seconds) (speedup)} \\ \hline
      &            & reg       & shred-inc       & shred-acc       \\ \hline
Q1    & 5          & 52.1      & 20.9 (2.5)      & 22.2 (2.3)      \\ \hline
Q1    & 10         & 84.1      & 29.9 (2.8)      & 34.9 (2.4)      \\ \hline
Q1    & 25         & 196       & 53.1 (3.7)      & 55.0 (3.6)      \\ \hline
Q2    & 5          & 57.4      & 70.3 (0.82)     & 71.5 (0.80)     \\ \hline
Q2    & 10         & 103       & 118 (0.87)      & 121 (0.85)      \\ \hline
Q2    & 25         & 246       & 257 (0.96)      & 270 (0.91)      \\ \hline
Q3    & 5          & 107       & 83.0 (1.29)     & 83.8 (1.28)     \\ \hline
Q3    & 10         & 189       & 131 (1.44)      & 135 (1.40)      \\ \hline
Q3    & 25         & 431       & 282 (1.53)      & 289 (1.49)      \\ \hline
Q4    & 5          & 3.06      & 2.12 (1.44)     & 2.61 (1.17)     \\ \hline
Q4    & 10         & 5.76      & 3.66 (1.57)     & 5.18 (1.11)     \\ \hline
Q4    & 25         & 14.1      & 10.2 (1.38)     & 13.6 (1.04)     \\ \hline
\end{tabular}
\caption{Test results of complete queries.}
\label{mainresults}
\end{table}

We now present the test result for a set of complete queries, which create outputs with varying levels of nesting, and requiring varying numbers of joins. Note that as per the limitation discussed in ref{}, each query only streams one input table. The rest of the inputs are static RDDs.

The queries are detailed in Appendix \ref{}, but here we illustrate \textit{Q1}, showing how it was written using the DSL:
\vs\begin{lstlisting}
val partOrders = For((l_orderkey, l_partkey, __) <-- lineitem)
                        Yield( l_partkey, l_orderkey)
                        
val orderPartNames = Group(
   For ((__, orderKey0, partName0) <-- partOrders.join(part))
     Yield (orderKey0, partName0)
 )

val customerOrders =
  For ((__, partNames0, custKey0, orderDate0) <-- orderPartNames.join(orders))
    Yield (custKey0, orderDate0, partNames0)


val customerNameOrders = For (
  (__, orderDate1, partNames1, custName0, __) <-- customerOrders.join(customer)
) Yield (custName0, orderDate1, partNames1)

val query = Group(customerNameOrders)
\end{lstlisting}\vs

For these more expensive queries, we use a dataset constructed as per the isolated testing of group, but using a random sample of 1000 customers as the basis rather than 10000. We again test three methods of evaluation, regular, shredded, and accumulated-shredded.

The results are shown in figure \ref{mainresults}. For three out of four queries, shredding is faster, whether accumulated or not. Although in \textit{Q1}, it reaches 3.7 times faster than regular evaluation, for the other two it is not significantly faster. However, we highlight the very consistent pattern that shredding becomes faster relative to regular evaluation as the number of batches increases. This is a natural result of the fact that without shredding, recomputation is required with each batch, so each batch, no matter how small, triggers recomputation of the query for all previous batches.

It is also worth noting that although accumulating the shredding result is consistently slower than not, the cost is marginal for most queries, and so in environments where this is necessary shredding should still be more efficient than regular evaluation.

For \textit{Q2}, which shredding slows down, this is not necessarily surprising. Since we showed in the isolated testing of group that for some inputs, shredding group may only marginally speed it up, this could mean that the speed up is not worth the cost of creating and maintaining the dictionaries. However, it is notable that even for this query, the differential between regular and shredded evaluation lessens as the number of batches increases. It's possible that with sufficient batches shredding would indeed be faster, as recomputation costs began to outweigh the overheads of shredding.

