\begin{abstract}
Apache Spark is a parallel data processing framework with a generic collection API, allowing the distributed processing of any serialisable datatype. Though this allows the construction and processing of nested collections, severe performance issues are encountered when these nested collections exhibit skew due to the inability to distribute them among multiple worker nodes. Further, many nested queries are unable to be incrementalised efficiently. Attempts to rectify this often involve a technique called shredding, which decouples the nested collections from the main query and allows them to be distributed and updated separately. However, many of the shredding transformations proposed have limited scope in the queries to which they can be applied, and are not offered as programmatic tools which can automatically transform queries.

In this work we develop a variant of the Nested Relational Calculus (NRC) which supports a compositional and generic shredding transformation. We implement it with a Scala DSL, which allows NRC queries to be expressed with natural and minimal syntax. The DSL can shred and evaluate queries on local Scala collections, Spark RDDs and Spark DStreams, and is easily extensible to other data types. We also contribute several novel techniques for developing DSLs in Scala which are generally applicable to many similar tasks.

We obtain performance gains of up to [***] on shredded queries on Spark DStreams, although not all queries are sped up by shredding. We conclude that while shredding shows promise and this work contributes greatly to the ease of implementing such query transformations, further work would be necessary in order to enable shredding to be a worthwhile optimisation to integrate with Spark or other data processing frameworks.  
\end{abstract}