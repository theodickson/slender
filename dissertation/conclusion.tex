\chapter{Conclusion} \label{conclusion}

\section{Summary of contributions}

The project introduces a new variant of the nested relational calculus together with a definition of a shredding transform on its expressions. Along with this we contribute a fully functional type-safe Scala DSL for representing these expressions. These queries can be automatically evaluated, including using shredding, thus achieving the main goal of automating the shredding transformation.

The datatypes supported include local Scala collections, Spark RDDs and Spark DStreams, with nested collections to supported to unlimited depth. To the best of our knowledge, this is the first such end-to-end system for shredding queries in Spark, and thus this is altogether a significant step towards more efficient evaluation of nested queries in a distributed environment, both static and streamed.

We highlight in particular the various uses of advanced Scala features such as implicit resolution and implicit classes which harness the Scala compiler to allow the DSL to have minimal and natural syntax without sacrificing expressibility. These techniques are generally applicable not just to query translation frameworks but to Scala DSLs in general and so could form the basis of many related projects. Further, the modular nature of the typeclass-based design makes the DSL easily extensible to new datatypes, and simple to optimise by changing the implementations of the algebraic operators.

We also show some limited performance benefits for queries written in the DSL when shredded, compared to the standard evaluation method. In particular, we note that as the number of batches in a streamed query increases, the shredding becomes increasingly efficient relative to standard evaluation, thus indicating that in long-running streaming environments shredding could increase system throughput many times over.

\section{Limitations and future work} \label{limitations}

We highlight several key limitations of this work, and suggest future research directions to overcome these.

\vs Firstly, the constrained version of the NRC we introduced limits both the range of queries that can be represented and the ease with which they can be expressed.  The variant we based our calculus on, introduced in \cite{draftpaper}, uses a generic construct $toK$ to box arbitrary ring-typed expressions, thus allowing the creation of arbitrary key-nested expressions. This allows a more natural syntax for constructing and manipulating nested data, as demonstrated in \ref{nrclimit} with the alternate way of expressing the $group$ operator with a nested iteration.

However, we introduced the change to an explicit $group$ operator for pragmatic reasons, as we were unable to implement the evaluation of such nested iterations in Spark. It would thus be very useful to design a system for doing so. We suggest this might be possible by using two types of abstract syntax trees - one which represents queries written in the more general calculus, and one which represents queries in the calculus used here with nodes such as $join$ and $group$, corresponding to Spark primitives which can be directly evaluated. The challenge would then consist in designing a transformation from ASTs of the former type to ASTs of the latter.

\vs Second, we did not touch upon the re-nesting of queries. While it's likely that in many scenarios, the entire nested output would never be needed at one time (for example the streaming service query introduced in \ref{motivatingexample}), and so it is valid to compare system throughput for regular versus shredded evaluation without re-nesting, it would still be beneficial to have the option to automatically construct the nested output from the shredded output.

Although the structure of the shredded output corresponds to the flat output type in a deterministic way which allows the unambiguous resolution of labels in the flat output to their defining dictionaries, it is still an engineering challenge to implement such an interface. We suggest that similar techniques to our implementation of evaluation could be used. Specifically, consider a trait which abstracts a function from \lin{(Output,Context) => (NewOutput,NewContext)}. Each instance of this trait would consume the outermost dictionary in the shredding context, updating the output with the labels defined in that dictionary replaced with their nested collections. Instances of this could be chained together inductively using implicit resolution to create a single function to entirely recreate the fully nested output.

\vs It is also clear that more extensive testing needs to be carried out. Firstly, it would be useful to understand more precisely when shredding will be beneficial, and what exactly causes the underperformance when it is not. For example, whether this is related directly to the size or skew of the nested collections constructed, or more due to the downstream operations in the query.

The system also needs to be tested in proper distributed computing environment, in order to test properly the load-balancing benefits of shredding. Particular consideration in such experiments should be given to the skew of the dataset and how shredding can be tuned to work with varying levels of skew. We note the suggestion in \cite{draftpaper} of a system of partial shredding, which could analyse which inner collections should in fact be shredded depending on their skew and size.

It would also be useful to incorporate more general optimisation techniques. For example, when joining DStreams to RDDs, we already use a custom-implemented broadcast join as each batch is likely to be very small compared to the static RDD. However, there are many more decisions that could be made surrounding partitioning of RDDs, e.g. when to repartition and into how many partitions. These all affect join performance as well as performance of \lin{reduceByKey} operations, both of which our framework uses heavily.
For both partial shredding and the general optimisations, its likely that to implement these an initial stage would be necessary in order to calculate an estimate of the sizes and distributions of collections and inner collections, as in standard relational optimisers.

We suspect that in a higher-parallelism environment with more heavily skewed data and further optimisations implemented, the performance benefits of shredding would become more clear.

\vs Finally, it would be a big contribution to incorporate this system into Apache Spark itself or other data processing frameworks, so that there is no need to use the DSL to express queries. However, the RDD API is procedural and so it would be challenging to shred queries expressed as RDDs. We suggest switching the focus to Spark DataFrames instead, as queries on DataFrames are expressed as SQL-like declarative ASTs, which offers a more direct pathway to translate to NRC ASTs.

In addition, further integration with Spark would help toward the ability to perform more detailed profiling and analysis of the generated Spark queries. Although full details of the jobs generated can be inspected in the Spark logs, it is not obvious how to match this information with the structure of our abstract queries and/or the implementation details of the operators. Developing more detailed profiling methods would point towards parts of the evaluation which are exhibiting performance issues and thus aid attempts to optimise shredded evaluation further, helping to build toward a truly beneficial and generically applicable shredding transformation.
