\chapter{Conclusion} \label{conclusion}

\section{Summary of contributions}

The project introduces a new variant of the nested relational calculus together with a definition of a shredding transform on its expressions. Along with this we contribute a fully functional type-safe Scala DSL for representing these expressions. These queries can be automatically evaluated, including using shredding, thus achieving the main goal of automating the shredding transformation.

The datatypes supported include local Scala collections, Spark RDDs and Spark DStreams, with nested collections to unlimited depth. Altogether this is a big step towards more efficient evaluation of nested queries in a distributed environment, both static and streamed.

We highlight in particular the various uses of advanced Scala features such as implicit resolution and implicit classes which harness the Scala compiler to allow the DSL to have minimal and natural syntax without sacrificing expressibility. These techniques are generally applicable not just to query translation frameworks but to Scala DSLs in general and so could form the basis of many related projects. Further, the modular nature of the typeclass-based design makes the DSL easily extensible to new datatypes, and simple to optimise by changing the implementations of the algebraic operators.

We also show some limited performance benefits for queries written in the DSL when shredded, compared to the standard evaluation method. In particular, we note that as the number of batches in a streamed query increases, the shredding becomes increasingly efficient relative to standard evaluation, thus indicating that in long-running streaming environments shredding could increase system throughput many times over.

\section{Limitations and future work}

We highlight several key limitations with this work, and suggest future research directions to overcome these.

First, the constrained version of the NRC we introduced, while pragmatic, limits both the range of queries that can be represented and the ease with which they can be expressed.  It would thus be very useful to create techniques for evaluating the more general calculus in \cite{draftpaper}. We suggest this might be possible with two systems of abstract syntax trees - one which represents the queries written in the more general calculus, using the toK and fromK constructs, and one in the calculus used here with nodes such as join and group, which can be directly evaluated. The challenge would then consist in designing a optimiser which can translate the former ASTs into equivalent ASTs in the latter framework.

It is also clear that more extensive testing needs to be carried out. Firstly, it would be useful to understand more precisely when shredding will be beneficial, and what exactly causes the underperformance when it will not be beneficial. For example, whether this is related directly to the size or skew of the nested collections constructed, or more due to the downstream operations in the query.

The system also needs to be tested in proper distributed computing environment, in order to test properly the load-balancing benefits of shredding. Particular consideration in such work should given to the skew of the dataset and how shredding can be tuned to work with varying levels of skew. We note the suggestion in \cite{draftpaper} of a system of partial shredding, which could analyse which inner collections should in fact be shredded depending on their skew and size.
It would also be useful to incorporate more general optimisation techniques. For example, when joining DStreams to RDDs, we already use a custom-implemented broadcast join as each batch is likely to be very small compared to the static RDD. However, there are many more decisions that could be made surrounding partitioning of RDDs, e.g. when to repartition and into how many partitions. These all affect join performance as well as performance of \lin{reduceByKey} operations, both of which our framework uses heavily.
For both partial shredding and the general optimisation, its likely that to implement these an initial stage would be necessary in order to calculate an estimate of the sizes of the tables and their inner collections, as in standard relational optimisers.

Finally, it would be a big contribution to incorporate this system into Apache Spark itself or other data processing frameworks, so that there is no need to use the DSL to express queries. We suggest switching the focus to Spark DataFrames instead of RDDs, as queries on DataFrames are expressed in accessible ASTs themselves, which offers a more direct pathway to translate to NRC ASTs.
