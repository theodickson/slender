
\section{Motivating example} {

To demonstrate this issue, let us consider the following scenario. A streaming music service allows listeners to rate tracks as they listen to them, with either a like (+1) or a dislike (-1). These ratings are processed using the following structure of case classes:
\vs\begin{lstlisting}
case class Rating(artist: Artist, track: Track, value: Int)
case class Artist(id: Long, name: String)
case class Track(id: Long,  title: String)
\end{lstlisting}
The full dataset can therefore be processed in Spark via an \lstinline{RDD[Rating]}. The streaming service wishes to compute a new dataset containing, for every artist, the set of their tracks that have been rated in the past month and for each track its aggregate rating. They wish to store this data in nested form so that it can efficiently provide the results in JSON form for an endpoint in the service's public API. Therefore they must compute an \lstinline{RDD[(Artist,Map[Track,Int])]}. The following spark query is the standard way to compute this:
\vs\begin{lstlisting}
val ratings: RDD[Rating] //the input set of ratings
val trackRatingsByArtist: RDD[(Artist,Map[Track,Int])] =
  ratings.groupBy(_.artist) //group by artist
    .map { case (artist,ratings) => //for each artist
      (artist, //return the artist
       ratings.map { case Rating(_,track,value) => (track,value) } 
        .reduceByKey(_ + _)
	.toMap //and aggregate the total ratings for each track
	)
    }
\end{lstlisting}\vs
Firstly, this query is likely to exhibit severe load-balancing issues. This is because the initial grouping by artist will aggregate every rating for a given artist into a single row. However, due to the natural power-law dynamics of many social phenomena such as music popularity \cite{musicpowerlaw}, a small number of artists will represent a large proportion of the ratings. In particular, there will probably be some artists with hundreds of thousands or even millions of track ratings in the past month. Thus, the workers which happen to contain such rows will be more likely to run out of memory, potentially crashing the query, and even if they don't, the imbalance will lead to some workers finishing further processing of the ratings far sooner than others, which is an inefficient use of resources.

Secondly, this query cannot be \textit{efficiently incrementalised}. This means that its results cannot be updated with an efficient \textit{union} operation, given a new batch of data, without reprocessing the original output in any way. To demonstrate this, suppose that the music service instead wishes to calculate an up-to-date version of this dataset every hour, by processing an \lstinline{RDD} of the ratings for the last hour and combining this with the previous version. That is, we have:
\vs\begin{lstlisting}
val oldTrackRatingsByArtist: RDD[(Artist,Map[Track,Int])]
val ratingsDelta: RDD[Rating]
\end{lstlisting}\vs
and then calculate the above query \textit{trackRatingsByArtist} just for the latest ratings:
\vs\begin{lstlisting}
val trackRatingsByArtistDelta: RDD[(Artist,Map[Track,Int])]
\end{lstlisting}\vs
Then the RDD:
\vs\begin{lstlisting}
val oldTrackRatingsByArtist.union(trackRatingsByArtistDelta)
\end{lstlisting}\vs
is not the correctly up-to-date version. It will include duplicate entries for each artist appearing in both the old version and the latest ratings. 
}

\begin{table}[]
\begin{tabular}{|l|l|l|l|l|}
\hline
Query & \# batches & \multicolumn{3}{l|}{Time (seconds) (speedup)} \\ \hline
      &            & reg       & shred-inc       & shred-acc       \\ \hline
Q1    & 5          & 52.1      & 20.9 (2.5)      & 22.2 (2.3)      \\ \hline
Q1    & 10         & 84.1      & 29.9 (2.8)      & 34.9 (2.4)      \\ \hline
Q1    & 25         & 196       & 53.1 (3.7)      & 55.0 (3.6)      \\ \hline
Q2    & 5          & 57.4      & 70.3 (0.82)     & 71.5 (0.80)     \\ \hline
Q2    & 10         & 103       & 118 (0.87)      & 121 (0.85)      \\ \hline
Q2    & 25         & 246       & 257 (0.96)      & 270 (0.91)      \\ \hline
Q3    & 5          & 107       & 83.0 (1.29)     & 83.8 (1.28)     \\ \hline
Q3    & 10         & 189       & 131 (1.44)      & 135 (1.40)      \\ \hline
Q3    & 25         & 431       & 282 (1.53)      & 289 (1.49)      \\ \hline
Q4    & 5          & 3.06      & 2.12 (1.44)     & 2.61 (1.17)     \\ \hline
Q4    & 10         & 5.76      & 3.66 (1.57)     & 5.18 (1.11)     \\ \hline
Q4    & 25         & 14.1      & 10.2 (1.38)     & 13.6 (1.04)     \\ \hline
\end{tabular}
\caption{Test results of complete queries.}
\label{mainresults}
\end{table}